{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import impyute as impy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi=pd.read_csv('Myocardial infarction complications Database.csv')\n",
    "drop_columns = ['ID',\n",
    "               'R_AB_1_n', 'R_AB_2_n', 'R_AB_3_n', 'NA_R_1_n', 'NA_R_2_n',\n",
    "               'NA_R_3_n', 'NOT_NA_1_n', 'NOT_NA_2_n', 'NOT_NA_3_n',\n",
    "               'FIBR_PREDS', 'PREDS_TAH', 'JELUD_TAH', 'FIBR_JELUD', 'A_V_BLOK',\n",
    "               'OTEK_LANC', 'RAZRIV', 'DRESSLER', 'ZSN', 'REC_IM', 'P_IM_STEN']\n",
    "\n",
    "mi = mi.replace({'LET_IS': [i for i in range(1, 8)]}, 1)\n",
    "mi = mi.drop(columns=drop_columns)\n",
    "drop_columns = ['IBS_NASL','D_AD_KBRIG','S_AD_KBRIG','KFK_BLOOD' ]\n",
    "\n",
    "mi = mi.drop(columns=drop_columns)\n",
    "binary=['SEX',\n",
    " 'SIM_GIPERT',\n",
    " 'nr_11',\n",
    " 'nr_01',\n",
    " 'nr_02',\n",
    " 'nr_03',\n",
    " 'nr_04',\n",
    " 'nr_07',\n",
    " 'nr_08',\n",
    " 'np_01',\n",
    " 'np_04',\n",
    " 'np_05',\n",
    " 'np_07',\n",
    " 'np_08',\n",
    " 'np_09',\n",
    " 'np_10',\n",
    " 'endocr_01',\n",
    " 'endocr_02',\n",
    " 'endocr_03',\n",
    " 'zab_leg_01',\n",
    " 'zab_leg_02',\n",
    " 'zab_leg_03',\n",
    " 'zab_leg_04',\n",
    " 'zab_leg_06',\n",
    " 'O_L_POST',\n",
    " 'K_SH_POST',\n",
    " 'MP_TP_POST',\n",
    " 'SVT_POST',\n",
    " 'GT_POST',\n",
    " 'FIB_G_POST',\n",
    " 'IM_PG_P',\n",
    " 'ritm_ecg_p_01',\n",
    " 'ritm_ecg_p_02',\n",
    " 'ritm_ecg_p_04',\n",
    " 'ritm_ecg_p_06',\n",
    " 'ritm_ecg_p_07',\n",
    " 'ritm_ecg_p_08',\n",
    " 'n_r_ecg_p_01',\n",
    " 'n_r_ecg_p_02',\n",
    " 'n_r_ecg_p_03',\n",
    " 'n_r_ecg_p_04',\n",
    " 'n_r_ecg_p_05',\n",
    " 'n_r_ecg_p_06',\n",
    " 'n_r_ecg_p_08',\n",
    " 'n_r_ecg_p_09',\n",
    " 'n_r_ecg_p_10',\n",
    " 'n_p_ecg_p_01',\n",
    " 'n_p_ecg_p_03',\n",
    " 'n_p_ecg_p_04',\n",
    " 'n_p_ecg_p_05',\n",
    " 'n_p_ecg_p_06',\n",
    " 'n_p_ecg_p_07',\n",
    " 'n_p_ecg_p_08',\n",
    " 'n_p_ecg_p_09',\n",
    " 'n_p_ecg_p_10',\n",
    " 'n_p_ecg_p_11',\n",
    " 'n_p_ecg_p_12',\n",
    " 'fibr_ter_01',\n",
    " 'fibr_ter_02',\n",
    " 'fibr_ter_03',\n",
    " 'fibr_ter_05',\n",
    " 'fibr_ter_06',\n",
    " 'fibr_ter_07',\n",
    " 'fibr_ter_08',\n",
    " 'GIPO_K',\n",
    " 'GIPER_NA',\n",
    " 'NA_KB',\n",
    " 'NOT_NA_KB',\n",
    " 'LID_KB',\n",
    " 'NITR_S',\n",
    " 'LID_S_n',\n",
    " 'B_BLOK_S_n',\n",
    " 'ANT_CA_S_n',\n",
    " 'GEPAR_S_n',\n",
    " 'ASP_S_n',\n",
    " 'TIKL_S_n',\n",
    " 'TRENT_S_n']\n",
    "cato=['INF_ANAM',\n",
    " 'STENOK_AN',\n",
    " 'FK_STENOK',\n",
    " 'IBS_POST',\n",
    " 'GB',\n",
    " 'DLIT_AG',\n",
    " 'ZSN_A',\n",
    " 'ant_im',\n",
    " 'lat_im',\n",
    " 'inf_im',\n",
    " 'post_im',\n",
    " 'TIME_B_S']\n",
    "continuous=['AGE',\n",
    " 'S_AD_ORIT',\n",
    " 'D_AD_ORIT',\n",
    " 'K_BLOOD',\n",
    " 'NA_BLOOD',\n",
    " 'ALT_BLOOD',\n",
    " 'AST_BLOOD',\n",
    " 'L_BLOOD',\n",
    " 'ROE']\n",
    "numerical=cato+continuous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mi.loc[:, mi.columns!='LET_IS']\n",
    "y = mi['LET_IS']\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.6)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to find the imputation method that works best for our dataset\n",
    "# We will test the performace of a number of base classifiers against different methods and pick the best one\n",
    "# Base classiers\n",
    "neigh = KNeighborsClassifier()\n",
    "log = LogisticRegression(random_state=1,max_iter=10000)\n",
    "dt= DecisionTreeClassifier(random_state=1)\n",
    "svm=SVC(random_state=1)\n",
    "rf=RandomForestClassifier(random_state=1)\n",
    "classifers=[neigh,log,dt,svm,rf]\n",
    "classiferss=['KNN','Logistic Regression','Decision Tree','Support Vector Machine','Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/impute/_iterative.py:669: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/impute/_iterative.py:669: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n"
     ]
    }
   ],
   "source": [
    "#Method 1\n",
    "imp = IterativeImputer(random_state=1)\n",
    "X_train_imp=pd.DataFrame(imp.fit_transform(X_train),index=X_train.index,columns=X_train.columns)\n",
    "X_valid_imp=pd.DataFrame(imp.fit_transform(X_valid),index=X_valid.index,columns=X_valid.columns)\n",
    "\n",
    "m1=''\n",
    "for i in range(len(classifers)):\n",
    "    result=classifers[i].fit(X_train_imp,y_train)\n",
    "    y_pred=result.predict(X_valid_imp)\n",
    "    f1score=f1_score(y_valid,y_pred)\n",
    "    recall=recall_score(y_valid,y_pred)\n",
    "    m1=m1+str(\"%s: F1Score: %f Recall: %f\\n\"%(classiferss[i],f1score,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2\n",
    "imp = KNNImputer(n_neighbors=5)\n",
    "X_train_imp=pd.DataFrame(imp.fit_transform(X_train),index=X_train.index,columns=X_train.columns)\n",
    "X_valid_imp=pd.DataFrame(imp.fit_transform(X_valid),index=X_valid.index,columns=X_valid.columns)\n",
    "\n",
    "m2=''\n",
    "for i in range(len(classifers)):\n",
    "    result=classifers[i].fit(X_train_imp,y_train)\n",
    "    y_pred=result.predict(X_valid_imp)\n",
    "    f1score=f1_score(y_valid,y_pred)\n",
    "    recall=recall_score(y_valid,y_pred)\n",
    "    m2=m2+str(\"%s: F1Score: %f Recall: %f\\n\"%(classiferss[i],f1score,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 3\n",
    "X_train_imp=copy.deepcopy(X_train)\n",
    "X_valid_imp=copy.deepcopy(X_valid)\n",
    "\n",
    "X_train_imp.update(pd.DataFrame(impy.median(X_train[binary+cato].to_numpy()), index=X_train[binary+cato].index,columns=X_train[binary+cato].columns))\n",
    "X_train_imp.update(pd.DataFrame(impy.mice(X_train[continuous].to_numpy()), index=X_train[continuous].index,columns=X_train[continuous].columns))\n",
    "\n",
    "X_valid_imp.update(pd.DataFrame(impy.median(X_valid[binary+cato].to_numpy()), index=X_valid[binary+cato].index,columns=X_valid[binary+cato].columns))\n",
    "X_valid_imp.update(pd.DataFrame(impy.mice(X_valid[continuous].to_numpy()), index=X_valid[continuous].index,columns=X_valid[continuous].columns))\n",
    "\n",
    "m3=''\n",
    "for i in range(len(classifers)):\n",
    "    result=classifers[i].fit(X_train_imp,y_train)\n",
    "    y_pred=result.predict(X_valid_imp)\n",
    "    f1score=f1_score(y_valid,y_pred)\n",
    "    recall=recall_score(y_valid,y_pred)\n",
    "    m3=m3+str(\"%s: F1Score: %f Recall: %f\\n\"%(classiferss[i],f1score,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 4\n",
    "X_train_imp=copy.deepcopy(X_train)\n",
    "X_valid_imp=copy.deepcopy(X_valid)\n",
    "\n",
    "X_train_imp.update(pd.DataFrame(impy.median(X_train_imp[binary+cato].to_numpy()), index=X_train[binary+cato].index,columns=X_train[binary+cato].columns))\n",
    "X_train_imp.update(pd.DataFrame(impy.fast_knn(X_train_imp[continuous].to_numpy()), index=X_train[continuous].index,columns=X_train[continuous].columns))\n",
    "\n",
    "X_valid_imp.update(pd.DataFrame(impy.median(X_valid_imp[binary+cato].to_numpy()), index=X_valid[binary+cato].index,columns=X_valid[binary+cato].columns))\n",
    "X_valid_imp.update(pd.DataFrame(impy.fast_knn(X_valid_imp[continuous].to_numpy()), index=X_valid[continuous].index,columns=X_valid[continuous].columns))\n",
    "\n",
    "m4=''\n",
    "for i in range(len(classifers)):\n",
    "    result=classifers[i].fit(X_train_imp,y_train)\n",
    "    y_pred=result.predict(X_valid_imp)\n",
    "    f1score=f1_score(y_valid,y_pred)\n",
    "    recall=recall_score(y_valid,y_pred)\n",
    "    m4=m4+str(\"%s: F1Score: %f Recall: %f\\n\"%(classiferss[i],f1score,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 5\n",
    "X_train_imp=copy.deepcopy(X_train)\n",
    "X_valid_imp=copy.deepcopy(X_valid)\n",
    "\n",
    "\n",
    "X_train_imp.update(pd.DataFrame(impy.median(X_train_imp[binary+cato].to_numpy()), index=X_train[binary+cato].index,columns=X_train[binary+cato].columns))\n",
    "X_train_imp.update(pd.DataFrame(impy.em(X_train_imp[continuous].to_numpy()), index=X_train[continuous].index,columns=X_train[continuous].columns))\n",
    "\n",
    "X_valid_imp.update(pd.DataFrame(impy.median(X_valid_imp[binary+cato].to_numpy()), index=X_valid[binary+cato].index,columns=X_valid[binary+cato].columns))\n",
    "X_valid_imp.update(pd.DataFrame(impy.em(X_valid_imp[continuous].to_numpy()), index=X_valid[continuous].index,columns=X_valid[continuous].columns))\n",
    "\n",
    "m5=''\n",
    "for i in range(len(classifers)):\n",
    "    result=classifers[i].fit(X_train_imp,y_train)\n",
    "    y_pred=result.predict(X_valid_imp)\n",
    "    f1score=f1_score(y_valid,y_pred)\n",
    "    recall=recall_score(y_valid,y_pred)\n",
    "    m5=m5+str(\"%s: F1Score: %f Recall: %f\\n\"%(classiferss[i],f1score,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method #1:\n",
      "KNN: F1Score: 0.275862 Recall: 0.184615\n",
      "Logistic Regression: F1Score: 0.560748 Recall: 0.461538\n",
      "Decision Tree: F1Score: 0.403361 Recall: 0.369231\n",
      "Support Vector Machine: F1Score: 0.115942 Recall: 0.061538\n",
      "Random Forest: F1Score: 0.329114 Recall: 0.200000\n",
      "\n",
      "Method #2:\n",
      "KNN: F1Score: 0.261905 Recall: 0.169231\n",
      "Logistic Regression: F1Score: 0.540000 Recall: 0.415385\n",
      "Decision Tree: F1Score: 0.459016 Recall: 0.430769\n",
      "Support Vector Machine: F1Score: 0.059701 Recall: 0.030769\n",
      "Random Forest: F1Score: 0.311688 Recall: 0.184615\n",
      "\n",
      "Method #3:\n",
      "KNN: F1Score: 0.282609 Recall: 0.200000\n",
      "Logistic Regression: F1Score: 0.547170 Recall: 0.446154\n",
      "Decision Tree: F1Score: 0.439024 Recall: 0.415385\n",
      "Support Vector Machine: F1Score: 0.115942 Recall: 0.061538\n",
      "Random Forest: F1Score: 0.285714 Recall: 0.169231\n",
      "\n",
      "Method #4:\n",
      "KNN: F1Score: 0.285714 Recall: 0.200000\n",
      "Logistic Regression: F1Score: 0.524272 Recall: 0.415385\n",
      "Decision Tree: F1Score: 0.434109 Recall: 0.430769\n",
      "Support Vector Machine: F1Score: 0.115942 Recall: 0.061538\n",
      "Random Forest: F1Score: 0.263158 Recall: 0.153846\n",
      "\n",
      "Method #5:\n",
      "KNN: F1Score: 0.246914 Recall: 0.153846\n",
      "Logistic Regression: F1Score: 0.509804 Recall: 0.400000\n",
      "Decision Tree: F1Score: 0.446281 Recall: 0.415385\n",
      "Support Vector Machine: F1Score: 0.088235 Recall: 0.046154\n",
      "Random Forest: F1Score: 0.240000 Recall: 0.138462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Method #1:\\n\"+m1)\n",
    "print(\"Method #2:\\n\"+m2)\n",
    "print(\"Method #3:\\n\"+m3)\n",
    "print(\"Method #4:\\n\"+m4)\n",
    "print(\"Method #5:\\n\"+m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preperation to see if any Method is statistically significantly better\n",
    "m1g=[m1.split(' ')[2],m1.split(' ')[7],m1.split(' ')[12],m1.split(' ')[18],m1.split(' ')[23]]\n",
    "m2g=[m2.split(' ')[2],m2.split(' ')[7],m2.split(' ')[12],m2.split(' ')[18],m2.split(' ')[23]]\n",
    "m3g=[m3.split(' ')[2],m3.split(' ')[7],m3.split(' ')[12],m3.split(' ')[18],m3.split(' ')[23]]\n",
    "m4g=[m4.split(' ')[2],m4.split(' ')[7],m4.split(' ')[12],m4.split(' ')[18],m4.split(' ')[23]]\n",
    "m5g=[m5.split(' ')[2],m5.split(' ')[7],m5.split(' ')[12],m5.split(' ')[18],m5.split(' ')[23]]\n",
    "bg = [ float(x) for x in bg ]\n",
    "m1g = [ float(x) for x in m1g ]\n",
    "m2g = [ float(x) for x in m2g ]\n",
    "m3g = [ float(x) for x in m3g ]\n",
    "m4g = [ float(x) for x in m4g ]\n",
    "m5g = [ float(x) for x in m5g ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34254747982605815\n",
      "          0         1         2         3         4\n",
      "0  1.000000  0.900000  0.900000  0.900000  0.374437\n",
      "1  0.900000  1.000000  0.900000  0.900000  0.724319\n",
      "2  0.900000  0.900000  1.000000  0.900000  0.374437\n",
      "3  0.900000  0.900000  0.900000  1.000000  0.724319\n",
      "4  0.374437  0.724319  0.374437  0.724319  1.000000\n"
     ]
    }
   ],
   "source": [
    "#Stastical Significance of methods\n",
    "groups=[m1g,m2g,m3g,m4g,m5g]\n",
    "#Check if Friedman test is signifiant\n",
    "chi_square,p_value_mean=stats.friedmanchisquare(*groups)\n",
    "print(p_value_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is shown that each method is not significantly better than any other method. Although it is shown that Method 3 has the broadest improvement over all classifiers. We will chose this method to proceed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
