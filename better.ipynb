{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import impyute as impy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import copy\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "import caffeine\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.loc[:, train.columns!='LET_IS']\n",
    "y_train = train['LET_IS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary=['SEX',\n",
    " 'SIM_GIPERT',\n",
    " 'nr_11',\n",
    " 'nr_01',\n",
    " 'nr_02',\n",
    " 'nr_03',\n",
    " 'nr_04',\n",
    " 'nr_07',\n",
    " 'nr_08',\n",
    " 'np_01',\n",
    " 'np_04',\n",
    " 'np_05',\n",
    " 'np_07',\n",
    " 'np_08',\n",
    " 'np_09',\n",
    " 'np_10',\n",
    " 'endocr_01',\n",
    " 'endocr_02',\n",
    " 'endocr_03',\n",
    " 'zab_leg_01',\n",
    " 'zab_leg_02',\n",
    " 'zab_leg_03',\n",
    " 'zab_leg_04',\n",
    " 'zab_leg_06',\n",
    " 'O_L_POST',\n",
    " 'K_SH_POST',\n",
    " 'MP_TP_POST',\n",
    " 'SVT_POST',\n",
    " 'GT_POST',\n",
    " 'FIB_G_POST',\n",
    " 'IM_PG_P',\n",
    " 'ritm_ecg_p_01',\n",
    " 'ritm_ecg_p_02',\n",
    " 'ritm_ecg_p_04',\n",
    " 'ritm_ecg_p_06',\n",
    " 'ritm_ecg_p_07',\n",
    " 'ritm_ecg_p_08',\n",
    " 'n_r_ecg_p_01',\n",
    " 'n_r_ecg_p_02',\n",
    " 'n_r_ecg_p_03',\n",
    " 'n_r_ecg_p_04',\n",
    " 'n_r_ecg_p_05',\n",
    " 'n_r_ecg_p_06',\n",
    " 'n_r_ecg_p_08',\n",
    " 'n_r_ecg_p_09',\n",
    " 'n_r_ecg_p_10',\n",
    " 'n_p_ecg_p_01',\n",
    " 'n_p_ecg_p_03',\n",
    " 'n_p_ecg_p_04',\n",
    " 'n_p_ecg_p_05',\n",
    " 'n_p_ecg_p_06',\n",
    " 'n_p_ecg_p_07',\n",
    " 'n_p_ecg_p_08',\n",
    " 'n_p_ecg_p_09',\n",
    " 'n_p_ecg_p_10',\n",
    " 'n_p_ecg_p_11',\n",
    " 'n_p_ecg_p_12',\n",
    " 'fibr_ter_01',\n",
    " 'fibr_ter_02',\n",
    " 'fibr_ter_03',\n",
    " 'fibr_ter_05',\n",
    " 'fibr_ter_06',\n",
    " 'fibr_ter_07',\n",
    " 'fibr_ter_08',\n",
    " 'GIPO_K',\n",
    " 'GIPER_NA',\n",
    " 'NA_KB',\n",
    " 'NOT_NA_KB',\n",
    " 'LID_KB',\n",
    " 'NITR_S',\n",
    " 'LID_S_n',\n",
    " 'B_BLOK_S_n',\n",
    " 'ANT_CA_S_n',\n",
    " 'GEPAR_S_n',\n",
    " 'ASP_S_n',\n",
    " 'TIKL_S_n',\n",
    " 'TRENT_S_n']\n",
    "cato=['INF_ANAM',\n",
    " 'STENOK_AN',\n",
    " 'FK_STENOK',\n",
    " 'IBS_POST',\n",
    " 'GB',\n",
    " 'DLIT_AG',\n",
    " 'ZSN_A',\n",
    " 'ant_im',\n",
    " 'lat_im',\n",
    " 'inf_im',\n",
    " 'post_im',\n",
    " 'TIME_B_S']\n",
    "continuous=['AGE',\n",
    " 'S_AD_ORIT',\n",
    " 'D_AD_ORIT',\n",
    " 'K_BLOOD',\n",
    " 'NA_BLOOD',\n",
    " 'ALT_BLOOD',\n",
    " 'AST_BLOOD',\n",
    " 'L_BLOOD',\n",
    " 'ROE']\n",
    "numerical=cato+continuous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the imputation step; made in the other code.\n",
    "X_train.update(pd.DataFrame(impy.median(X_train[binary+cato].to_numpy()), index=X_train[binary+cato].index,columns=X_train[binary+cato].columns))\n",
    "X_train.update(pd.DataFrame(impy.mice(X_train[continuous].to_numpy()), index=X_train[continuous].index,columns=X_train[continuous].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "Using the outcomes of the base classifiers in the imputation section of our code; we have decided to tune a Logistic Regression Classifier, and K Nearest Neighbor Classifier. We also want to try the ensemble learner XGBoost to see if it can also provide good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to use Stratified K Fold for our dataset because we have many features that are unbalanced.\n",
    "#Stratified K Fold makes sure that each class is represented.\n",
    "#This cross validation split will be used to iterate over multiple RandomizedSearchCV or GridSearchCV\n",
    "cv_outer= StratifiedKFold(n_splits=15, shuffle=True, random_state=1)\n",
    "#This cross validation split will be used within the RandomizedSearchCV and GridSearchCV\n",
    "cv_inner= StratifiedKFold(n_splits=25, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is where we initilize the XGBoost Classifier, set optimization parameters not hyperparameters\n",
    "#Objective has three options:multi:softmax,multi:softprob, binary:logistic\n",
    "#We use binary:logistic because our target feature is binary.\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(objective='binary:logistic',eval_metric='logloss', use_label_encoder=False,seed=1)\n",
    "\n",
    "xgb_pipeline = Pipeline([ \n",
    "    ('model_xgb', model_xgb)\n",
    "])\n",
    "\n",
    "# The search space for hyperparameters were found in xgboost documentation and some help for this webpage: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "xgb_param= {\n",
    "    'model_xgb__max_depth': range(2,20),\n",
    "    'model_xgb__min_child_weight':range(1,10),\n",
    "    'model_xgb__gamma':[i/10.0 for i in range(0,5)],\n",
    "    'model_xgb__subsample':[i/10.0 for i in range(6,10)],\n",
    "    'model_xgb__colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "    'model_xgb__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "    }\n",
    "xgb_random = RandomizedSearchCV(xgb_pipeline, xgb_param, cv=cv_inner, scoring='f1', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression \n",
    "log = LogisticRegression(random_state=0)\n",
    "log_pipeline=Pipeline([  \n",
    "    ('log',log)\n",
    "])\n",
    "#The hyperparameter penalty heavly influences the rest of the choices because most of the solvers can not deal with l2 penality.\n",
    "#We thought the l2 was something to see if it worked with our dataset so we choose to include it \n",
    "log_param={\n",
    "            'log__penalty':['l1', 'l2'],\n",
    "            'log__solver':['liblinear','saga'],\n",
    "            'log__C':[0.001,0.01,0.1,1,10,100, 1000],\n",
    "            'log__max_iter':[10000],\n",
    "            'log__fit_intercept':[True,False],\n",
    "}\n",
    "\n",
    "log_random=RandomizedSearchCV(log_pipeline, log_param, cv=cv_inner, scoring='f1', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Nearest Neighbor Classifier\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh_pipeline = Pipeline([ \n",
    "    ('neigh', neigh)\n",
    "])\n",
    "#These hyperparaters were chosen from the sklearn documentation\n",
    "# metric and metric_params were excluded because of lack of understanding of there use\n",
    "#n_jobs is said in the documentation to not affect fit, and only affect computation\n",
    "neigh_param = {\n",
    "                'neigh__n_neighbors': range(1,round(len(X_train)*0.6)),\n",
    "                'neigh__leaf_size': range(len(X_train)),\n",
    "                'neigh__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                'neigh__weights': ['uniform', 'distance'],\n",
    "                'neigh__p': [1,2],\n",
    "                \n",
    "}\n",
    "\n",
    "neigh_random = RandomizedSearchCV(neigh_pipeline, neigh_param, cv=cv_inner, scoring='f1', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "XGBoost F-1 Score: 0.380952 Recall: 0.285714 AUC: 0.623377\n",
      "Logistic Regression F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.659091\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.117647 Recall: 0.071429 AUC: 0.522727\n",
      "Iteration 2\n",
      "XGBoost F-1 Score: 0.416667 Recall: 0.357143 AUC: 0.646104\n",
      "Logistic Regression F-1 Score: 0.461538 Recall: 0.428571 AUC: 0.675325\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.300000 Recall: 0.214286 AUC: 0.587662\n",
      "Iteration 3\n",
      "XGBoost F-1 Score: 0.434783 Recall: 0.357143 AUC: 0.652597\n",
      "Logistic Regression F-1 Score: 0.583333 Recall: 0.500000 AUC: 0.730519\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.352941 Recall: 0.214286 AUC: 0.607143\n",
      "Iteration 4\n",
      "XGBoost F-1 Score: 0.454545 Recall: 0.333333 AUC: 0.653509\n",
      "Logistic Regression F-1 Score: 0.260870 Recall: 0.200000 AUC: 0.567105\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.222222 Recall: 0.133333 AUC: 0.560088\n",
      "Iteration 5\n",
      "XGBoost F-1 Score: 0.666667 Recall: 0.533333 AUC: 0.760088\n",
      "Logistic Regression F-1 Score: 0.500000 Recall: 0.400000 AUC: 0.680263\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.117647 Recall: 0.066667 AUC: 0.526754\n",
      "Iteration 6\n",
      "XGBoost F-1 Score: 0.620690 Recall: 0.600000 AUC: 0.767105\n",
      "Logistic Regression F-1 Score: 0.727273 Recall: 0.800000 AUC: 0.860526\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.571429 Recall: 0.400000 AUC: 0.700000\n",
      "Iteration 7\n",
      "XGBoost F-1 Score: 0.545455 Recall: 0.400000 AUC: 0.693421\n",
      "Logistic Regression F-1 Score: 0.666667 Recall: 0.600000 AUC: 0.780263\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.315789 Recall: 0.200000 AUC: 0.593421\n",
      "Iteration 8\n",
      "XGBoost F-1 Score: 0.545455 Recall: 0.400000 AUC: 0.693421\n",
      "Logistic Regression F-1 Score: 0.538462 Recall: 0.466667 AUC: 0.707018\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.421053 Recall: 0.266667 AUC: 0.633333\n",
      "Iteration 9\n",
      "XGBoost F-1 Score: 0.434783 Recall: 0.333333 AUC: 0.646930\n",
      "Logistic Regression F-1 Score: 0.538462 Recall: 0.466667 AUC: 0.707018\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.125000 Recall: 0.066667 AUC: 0.533333\n",
      "Iteration 10\n",
      "XGBoost F-1 Score: 0.434783 Recall: 0.333333 AUC: 0.646930\n",
      "Logistic Regression F-1 Score: 0.480000 Recall: 0.400000 AUC: 0.673684\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.125000 Recall: 0.066667 AUC: 0.533333\n",
      "Iteration 11\n",
      "XGBoost F-1 Score: 0.400000 Recall: 0.357143 AUC: 0.639098\n",
      "Logistic Regression F-1 Score: 0.444444 Recall: 0.428571 AUC: 0.668233\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.117647 Recall: 0.071429 AUC: 0.522556\n",
      "Iteration 12\n",
      "XGBoost F-1 Score: 0.608696 Recall: 0.500000 AUC: 0.736842\n",
      "Logistic Regression F-1 Score: 0.560000 Recall: 0.500000 AUC: 0.723684\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.133333 Recall: 0.071429 AUC: 0.535714\n",
      "Iteration 13\n",
      "XGBoost F-1 Score: 0.545455 Recall: 0.428571 AUC: 0.701128\n",
      "Logistic Regression F-1 Score: 0.666667 Recall: 0.642857 AUC: 0.795113\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.352941 Recall: 0.214286 AUC: 0.607143\n",
      "Iteration 14\n",
      "XGBoost F-1 Score: 0.560000 Recall: 0.500000 AUC: 0.723684\n",
      "Logistic Regression F-1 Score: 0.571429 Recall: 0.428571 AUC: 0.707707\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.250000 Recall: 0.142857 AUC: 0.571429\n",
      "Iteration 15\n",
      "XGBoost F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.658835\n",
      "Logistic Regression F-1 Score: 0.521739 Recall: 0.428571 AUC: 0.694549\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.125000 Recall: 0.071429 AUC: 0.529135\n",
      "Logistic Regression outer test: recall mean: 0.570070 (std: 0.146061)\n",
      "XGB outer test: recall mean: 0.529394 (std: 0.137578)\n",
      "K Nearest Neighbor Classifier​ outer test: recall mean: 0.321841 (std: 0.205808)\n"
     ]
    }
   ],
   "source": [
    "#This block of code is our first attempt to find the best hyperparameters and how those preform on the training dataset\n",
    "caffeine.on(display=True)\n",
    "xgb_outer_results = list()\n",
    "xgb_best_recall=float('-inf')\n",
    "xgb_best_parameters={}\n",
    "\n",
    "log_outer_results = list()\n",
    "log_best_recall=float('-inf')\n",
    "log_best_parameters={}\n",
    "\n",
    "knn_outer_results = list()\n",
    "knn_best_recall=float('-inf')\n",
    "knn_best_parameters={}\n",
    "\n",
    "iter_num=1\n",
    "for train_ix, test_ix in cv_outer.split(X_train, y_train):\n",
    "    print('Iteration',iter_num)\n",
    "    iter_num +=1\n",
    "    # Split data for inner \n",
    "    X_traini, X_testi = X_train.iloc[train_ix, :], X_train.iloc[test_ix, :]\n",
    "    y_traini, y_testi = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "    #XGBoost\n",
    "    result=xgb_random.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= xgb_best_recall:\n",
    "        xgb_best_recall=recall\n",
    "        xgb_best_parameters=result.best_params_\n",
    "    xgb_outer_results.append([recall,f1score,auc])\n",
    "    print(\"XGBoost F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "    #Logistic Regression\n",
    "    result=log_random.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= log_best_recall:\n",
    "        log_best_recall=recall\n",
    "        log_best_parameters=result.best_params_\n",
    "    log_outer_results.append([recall,f1score,auc])\n",
    "    print(\"Logistic Regression F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "    #K Nearest Neighbor Classifier\n",
    "    result=neigh_random.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= knn_best_recall:\n",
    "        knn_best_recall=recall\n",
    "        knn_best_parameters=result.best_params_\n",
    "    knn_outer_results.append([recall,f1score,auc])\n",
    "    print(\"K Nearest Neighbor Classifier F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "log_mean=np.mean(log_outer_results)\n",
    "xgb_mean=np.mean(xgb_outer_results)\n",
    "knn_mean=np.mean(knn_outer_results)\n",
    "print('Logistic Regression outer test: recall mean: %f (std: %f)' % (log_mean, np.std(log_outer_results)))\n",
    "print('XGB outer test: recall mean: %f (std: %f)' % (xgb_mean, np.std(xgb_outer_results)))\n",
    "print('K Nearest Neighbor Classifier​ outer test: recall mean: %f (std: %f)' % (knn_mean, np.std(knn_outer_results)))\n",
    "caffeine.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaulttParm={}\n",
    "for k in xgb_best_parameters.keys():\n",
    "    if 'model_xgb' in k:\n",
    "        parm=k.split('__')[1]\n",
    "        defaulttParm[parm]=xgb_best_parameters[k]\n",
    "defaulttParm['use_label_encoder']=False\n",
    "myFinalModel=xgb.XGBClassifier(**defaulttParm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=0.4, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=1e-05,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=0.7,\n",
      "              tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "print(myFinalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As can be seen in the results above XGBoost was a model that preformed very well.\n",
    "# We wanted to see is an exaustive grid search around the outcomes of previous best model could continue to improve results\n",
    "model_xgb = xgb.XGBClassifier(objective='binary:logistic',eval_metric='logloss', use_label_encoder=False,seed=1)\n",
    "\n",
    "xgb_pipeline = Pipeline([ \n",
    "    ('model_xgb', model_xgb)\n",
    "])\n",
    "\n",
    "# The search space for hyperparameters were found in xgboost documentation and some help for this webpage: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "xgb_param= {\n",
    "    'model_xgb__max_depth': range(5,10),\n",
    "    'model_xgb__min_child_weight':range(1,4),\n",
    "    'model_xgb__gamma':[i/10.0 for i in range(3,6)],\n",
    "    'model_xgb__subsample':[i/10.0 for i in range(6,8)],\n",
    "    'model_xgb__colsample_bytree':[i/10.0 for i in range(8,10)],\n",
    "    'model_xgb__reg_alpha':[1e-6,1e-5, 1e-4],\n",
    "    }\n",
    "\n",
    "xgb_exhaustive = GridSearchCV(xgb_pipeline, xgb_param, cv=cv_inner, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "caffeine.on(display=True)\n",
    "xgb2_outer_results = list()\n",
    "xgb2_best_recall=float('-inf')\n",
    "xgb2_best_parameters={}\n",
    "iter_num=1\n",
    "for train_ix, test_ix in cv_outer.split(X_train, y_train):\n",
    "    print('Iteration',iter_num)\n",
    "    iter_num +=1\n",
    "    # Split data for inner \n",
    "    X_traini, X_testi = X_train.iloc[train_ix, :], X_train.iloc[test_ix, :]\n",
    "    y_traini, y_testi = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "    #XGBoost\n",
    "    result=xgb_exhaustive.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= xgb2_best_recall:\n",
    "        xgb2_best_recall=recall\n",
    "        xgb2_best_parameters=result.best_params_\n",
    "    xgb2_outer_results.append([recall,f1score,auc])\n",
    "    print(\"XGBoost F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "xgb2_mean=np.mean(xgb2_outer_results)\n",
    "print('XGB outer test: recall mean: %f (std: %f)' % (xgb2_mean, np.std(xgb2_outer_results)))\n",
    "caffeine.off()\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.8,\n",
      "              enable_categorical=False, gamma=0.4, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=1e-06,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=0.6,\n",
      "              tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "defaulttParm2={}\n",
    "for k in xgb2_best_parameters.keys():\n",
    "    if 'model_xgb' in k:\n",
    "        parm=k.split('__')[1]\n",
    "        defaulttParm2[parm]=xgb2_best_parameters[k]\n",
    "defaulttParm2['use_label_encoder']=False\n",
    "myFinalModel2=xgb.XGBClassifier(**defaulttParm2)\n",
    "print(myFinalModel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was our first attempts at finding the best models and getting an understanding of how well this data could be classified. After our feature selection analysis we wanted to see if the preformace was improved we only using the feature of most importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell just takes out our feature that we selected\n",
    "feature_filter=['AGE',\n",
    " 'INF_ANAM',\n",
    " 'STENOK_AN',\n",
    " 'FK_STENOK',\n",
    " 'IBS_POST',\n",
    " 'GB',\n",
    " 'DLIT_AG',\n",
    " 'ZSN_A',\n",
    " 'nr_03',\n",
    " 'nr_08',\n",
    " 'endocr_03',\n",
    " 'S_AD_ORIT',\n",
    " 'D_AD_ORIT',\n",
    " 'O_L_POST',\n",
    " 'K_SH_POST',\n",
    " 'ant_im',\n",
    " 'lat_im',\n",
    " 'inf_im',\n",
    " 'IM_PG_P',\n",
    " 'ritm_ecg_p_01',\n",
    " 'ritm_ecg_p_07',\n",
    " 'n_r_ecg_p_08',\n",
    " 'n_p_ecg_p_01',\n",
    " 'n_p_ecg_p_08',\n",
    " 'n_p_ecg_p_12',\n",
    " 'K_BLOOD',\n",
    " 'NA_BLOOD',\n",
    " 'TIME_B_S',\n",
    " 'NA_KB',\n",
    " 'LID_KB']\n",
    "X_train_fs=X_train[feature_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In our initial analysis these important steps were missed.\n",
    "#For this section of the hyperparameter optimization we scale our continous data and get dummies for our categorical data that is not binary.\n",
    "scalor=StandardScaler()\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updates lists to match feature selection\n",
    "continuous.remove('ALT_BLOOD')\n",
    "continuous.remove('AST_BLOOD')\n",
    "continuous.remove('L_BLOOD')\n",
    "continuous.remove('ROE')\n",
    "cato.remove('post_im')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Dummies appends new columns to the data set to help work with categories to improve the performace of models\n",
    "encoder.fit(X_train_fs[cato])    \n",
    "enc_df = pd.DataFrame(encoder.fit_transform(X_train_fs[cato]).toarray(),index=X_train_fs.index)\n",
    "X_train_fs=X_train_fs.drop(columns=cato)\n",
    "X_train_fs=X_train_fs.join(enc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "XGBoost F-1 Score: 0.444444 Recall: 0.428571 AUC: 0.668831\n",
      "Logistic Regression F-1 Score: 0.416667 Recall: 0.357143 AUC: 0.646104\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 2\n",
      "XGBoost F-1 Score: 0.320000 Recall: 0.285714 AUC: 0.597403\n",
      "Logistic Regression F-1 Score: 0.400000 Recall: 0.285714 AUC: 0.629870\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.250000 Recall: 0.142857 AUC: 0.571429\n",
      "Iteration 3\n",
      "XGBoost F-1 Score: 0.608696 Recall: 0.500000 AUC: 0.737013\n",
      "Logistic Regression F-1 Score: 0.545455 Recall: 0.428571 AUC: 0.701299\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 4\n",
      "XGBoost F-1 Score: 0.272727 Recall: 0.200000 AUC: 0.573684\n",
      "Logistic Regression F-1 Score: 0.454545 Recall: 0.333333 AUC: 0.653509\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.125000 Recall: 0.066667 AUC: 0.533333\n",
      "Iteration 5\n",
      "XGBoost F-1 Score: 0.272727 Recall: 0.200000 AUC: 0.573684\n",
      "Logistic Regression F-1 Score: 0.380952 Recall: 0.266667 AUC: 0.620175\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 6\n",
      "XGBoost F-1 Score: 0.758621 Recall: 0.733333 AUC: 0.846930\n",
      "Logistic Regression F-1 Score: 0.689655 Recall: 0.666667 AUC: 0.807018\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 7\n",
      "XGBoost F-1 Score: 0.416667 Recall: 0.333333 AUC: 0.640351\n",
      "Logistic Regression F-1 Score: 0.608696 Recall: 0.466667 AUC: 0.726754\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 8\n",
      "XGBoost F-1 Score: 0.480000 Recall: 0.400000 AUC: 0.673684\n",
      "Logistic Regression F-1 Score: 0.545455 Recall: 0.400000 AUC: 0.693421\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.125000 Recall: 0.066667 AUC: 0.533333\n",
      "Iteration 9\n",
      "XGBoost F-1 Score: 0.454545 Recall: 0.333333 AUC: 0.653509\n",
      "Logistic Regression F-1 Score: 0.363636 Recall: 0.266667 AUC: 0.613596\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 10\n",
      "XGBoost F-1 Score: 0.428571 Recall: 0.400000 AUC: 0.653947\n",
      "Logistic Regression F-1 Score: 0.400000 Recall: 0.333333 AUC: 0.633772\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 11\n",
      "XGBoost F-1 Score: 0.434783 Recall: 0.357143 AUC: 0.652256\n",
      "Logistic Regression F-1 Score: 0.480000 Recall: 0.428571 AUC: 0.681391\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.133333 Recall: 0.071429 AUC: 0.535714\n",
      "Iteration 12\n",
      "XGBoost F-1 Score: 0.666667 Recall: 0.500000 AUC: 0.750000\n",
      "Logistic Regression F-1 Score: 0.666667 Recall: 0.571429 AUC: 0.772556\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 13\n",
      "XGBoost F-1 Score: 0.560000 Recall: 0.500000 AUC: 0.723684\n",
      "Logistic Regression F-1 Score: 0.727273 Recall: 0.571429 AUC: 0.785714\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.250000 Recall: 0.142857 AUC: 0.571429\n",
      "Iteration 14\n",
      "XGBoost F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.658835\n",
      "Logistic Regression F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.658835\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.000000 Recall: 0.000000 AUC: 0.500000\n",
      "Iteration 15\n",
      "XGBoost F-1 Score: 0.434783 Recall: 0.357143 AUC: 0.652256\n",
      "Logistic Regression F-1 Score: 0.461538 Recall: 0.428571 AUC: 0.674812\n",
      "K Nearest Neighbor Classifier F-1 Score: 0.133333 Recall: 0.071429 AUC: 0.535714\n",
      "Logistic Regression outer test: recall mean: 0.534574 (std: 0.151591)\n",
      "XGB outer test: recall mean: 0.509990 (std: 0.163187)\n",
      "K Nearest Neighbor Classifier outer test: recall mean: 0.207989 (std: 0.228569)\n",
      "Time:  1782.524999686997\n"
     ]
    }
   ],
   "source": [
    "#This is the second attempt to optimize hyperparameters this time using feature selcected data, scaling, and get dummies.\n",
    "start = timeit.default_timer()\n",
    "caffeine.on(display=True)\n",
    "xgb3_outer_results = list()\n",
    "xgb3_best_recall=float('-inf')\n",
    "xgb3_best_parameters={}\n",
    "\n",
    "log3_outer_results = list()\n",
    "log3_best_recall=float('-inf')\n",
    "log3_best_parameters={}\n",
    "\n",
    "knn3_outer_results = list()\n",
    "knn3_best_recall=float('-inf')\n",
    "knn3_best_parameters={}\n",
    "\n",
    "iter_num=1\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X_train_fs, y_train):\n",
    "    print('Iteration',iter_num)\n",
    "    iter_num +=1\n",
    "    # Split data for inner \n",
    "    X_traini, X_testi = X_train_fs.iloc[train_ix, :], X_train_fs.iloc[test_ix, :]\n",
    "    y_traini, y_testi = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "    #Scaling numeric features\n",
    "    scalor.fit(X_traini[continuous])\n",
    "    X_traini.update(pd.DataFrame(scalor.transform(X_traini[continuous]),index=X_traini[continuous].index,columns=continuous))\n",
    "    X_testi.update(pd.DataFrame(scalor.transform(X_testi[continuous]),index=X_testi[continuous].index,columns=continuous))\n",
    "    \n",
    "    #XGBoost\n",
    "    result=xgb_random.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= xgb_best_recall:\n",
    "        xgb3_best_recall=recall\n",
    "        xgb3_best_parameters=result.best_params_\n",
    "    xgb3_outer_results.append([recall,f1score,auc])\n",
    "    print(\"XGBoost F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "    #Logistic Regression\n",
    "    result=log_random.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= log_best_recall:\n",
    "        log3_best_recall=recall\n",
    "        log3_best_parameters=result.best_params_\n",
    "    log3_outer_results.append([recall,f1score,auc])\n",
    "    print(\"Logistic Regression F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "    #K Nearest Neighbor Classifier\n",
    "    result=neigh_random.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= knn_best_recall:\n",
    "        knn3_best_recall=recall\n",
    "        knn3_best_parameters=result.best_params_\n",
    "    knn3_outer_results.append([recall,f1score,auc])\n",
    "    print(\"K Nearest Neighbor Classifier F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "log3_mean=np.mean(log3_outer_results)\n",
    "xgb3_mean=np.mean(xgb3_outer_results)\n",
    "knn3_mean=np.mean(knn3_outer_results)\n",
    "print('Logistic Regression outer test: recall mean: %f (std: %f)' % (log3_mean, np.std(log3_outer_results)))\n",
    "print('XGB outer test: recall mean: %f (std: %f)' % (xgb3_mean, np.std(xgb3_outer_results)))\n",
    "print('K Nearest Neighbor Classifier outer test: recall mean: %f (std: %f)' % (knn3_mean, np.std(knn3_outer_results)))\n",
    "caffeine.off()\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=0.1, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=2, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=0.01,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=0.9,\n",
      "              tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "#This block of code shows what xgboot model had the best preformace.\n",
    "defaulttParm3={}\n",
    "for k in xgb3_best_parameters.keys():\n",
    "    if 'model_xgb' in k:\n",
    "        parm=k.split('__')[1]\n",
    "        defaulttParm3[parm]=xgb3_best_parameters[k]\n",
    "defaulttParm3['use_label_encoder']=False\n",
    "myFinalModel3=xgb.XGBClassifier(**defaulttParm3)\n",
    "print(myFinalModel3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "XGBoost F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.659091\n",
      "Iteration 2\n",
      "XGBoost F-1 Score: 0.428571 Recall: 0.428571 AUC: 0.662338\n",
      "Iteration 3\n",
      "XGBoost F-1 Score: 0.560000 Recall: 0.500000 AUC: 0.724026\n",
      "Iteration 4\n",
      "XGBoost F-1 Score: 0.260870 Recall: 0.200000 AUC: 0.567105\n",
      "Iteration 5\n",
      "XGBoost F-1 Score: 0.521739 Recall: 0.400000 AUC: 0.686842\n",
      "Iteration 6\n",
      "XGBoost F-1 Score: 0.560000 Recall: 0.466667 AUC: 0.713596\n",
      "Iteration 7\n",
      "XGBoost F-1 Score: 0.583333 Recall: 0.466667 AUC: 0.720175\n",
      "Iteration 8\n",
      "XGBoost F-1 Score: 0.583333 Recall: 0.466667 AUC: 0.720175\n",
      "Iteration 9\n",
      "XGBoost F-1 Score: 0.521739 Recall: 0.400000 AUC: 0.686842\n",
      "Iteration 10\n",
      "XGBoost F-1 Score: 0.583333 Recall: 0.466667 AUC: 0.720175\n",
      "Iteration 11\n",
      "XGBoost F-1 Score: 0.333333 Recall: 0.285714 AUC: 0.603383\n",
      "Iteration 12\n",
      "XGBoost F-1 Score: 0.521739 Recall: 0.428571 AUC: 0.694549\n",
      "Iteration 13\n",
      "XGBoost F-1 Score: 0.571429 Recall: 0.428571 AUC: 0.707707\n",
      "Iteration 14\n",
      "XGBoost F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.658835\n",
      "Iteration 15\n",
      "XGBoost F-1 Score: 0.476190 Recall: 0.357143 AUC: 0.665414\n",
      "XGB outer test: recall mean: 0.524766 (std: 0.137445)\n",
      "Time:  31629.398579547997\n"
     ]
    }
   ],
   "source": [
    "#This is another exhaustive grid search for xgboost to improve the model found after the randomized grid search\n",
    "start = timeit.default_timer()\n",
    "caffeine.on(display=True)\n",
    "xgb4_outer_results = list()\n",
    "xgb4_best_recall=float('-inf')\n",
    "xgb4_best_parameters={}\n",
    "iter_num=1\n",
    "for train_ix, test_ix in cv_outer.split(X_train_fs, y_train):\n",
    "    print('Iteration',iter_num)\n",
    "    iter_num +=1\n",
    "    # Split data for inner \n",
    "    X_traini, X_testi = X_train.iloc[train_ix, :], X_train.iloc[test_ix, :]\n",
    "    y_traini, y_testi = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "    #Scaling numeric features\n",
    "    scalor.fit(X_traini[continuous])\n",
    "    X_traini.update(pd.DataFrame(scalor.transform(X_traini[continuous]),index=X_traini[continuous].index,columns=continuous))\n",
    "    X_testi.update(pd.DataFrame(scalor.transform(X_testi[continuous]),index=X_testi[continuous].index,columns=continuous))\n",
    "    #XGBoost\n",
    "    result=xgb_exhaustive.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= xgb2_best_recall:\n",
    "        xgb4_best_recall=recall\n",
    "        xgb4_best_parameters=result.best_params_\n",
    "    xgb4_outer_results.append([recall,f1score,auc])\n",
    "    print(\"XGBoost F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "xgb4_mean=np.mean(xgb4_outer_results)\n",
    "print('XGB outer test: recall mean: %f (std: %f)' % (xgb4_mean, np.std(xgb4_outer_results)))\n",
    "caffeine.off()\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, use_label_encoder=False,\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "defaulttParm4={}\n",
    "for k in xgb4_best_parameters.keys():\n",
    "    if 'model_xgb' in k:\n",
    "        parm=k.split('__')[1]\n",
    "        defaulttParm4[parm]=xgb4_best_parameters[k]\n",
    "defaulttParm4['use_label_encoder']=False\n",
    "myFinalModel4=xgb.XGBClassifier(**defaulttParm4)\n",
    "print(myFinalModel4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb4_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression also preformed well in the RamdomizedGridSearch and this exhaustive grid search looks to improve those results\n",
    "log = LogisticRegression(random_state=0)\n",
    "log_pipeline=Pipeline([  \n",
    "    ('log',log)\n",
    "])\n",
    "log_param={\n",
    "            'log__penalty':['l1', 'l2'],\n",
    "            'log__solver':['liblinear','saga'],\n",
    "            'log__C':[0.001,0.01,0.1,1,10,100, 1000],\n",
    "            'log__max_iter':[10000],\n",
    "            'log__fit_intercept':[True,False],\n",
    "}\n",
    "\n",
    "log_exhaustive = GridSearchCV(log_pipeline, log_param, cv=cv_inner, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Logistic Regression F-1 Score: 0.500000 Recall: 0.428571 AUC: 0.688312\n",
      "Iteration 2\n",
      "Logistic Regression F-1 Score: 0.400000 Recall: 0.285714 AUC: 0.629870\n",
      "Iteration 3\n",
      "Logistic Regression F-1 Score: 0.636364 Recall: 0.500000 AUC: 0.743506\n",
      "Iteration 4\n",
      "Logistic Regression F-1 Score: 0.454545 Recall: 0.333333 AUC: 0.653509\n",
      "Iteration 5\n",
      "Logistic Regression F-1 Score: 0.380952 Recall: 0.266667 AUC: 0.620175\n",
      "Iteration 6\n",
      "Logistic Regression F-1 Score: 0.689655 Recall: 0.666667 AUC: 0.807018\n",
      "Iteration 7\n",
      "Logistic Regression F-1 Score: 0.608696 Recall: 0.466667 AUC: 0.726754\n",
      "Iteration 8\n",
      "Logistic Regression F-1 Score: 0.545455 Recall: 0.400000 AUC: 0.693421\n",
      "Iteration 9\n",
      "Logistic Regression F-1 Score: 0.363636 Recall: 0.266667 AUC: 0.613596\n",
      "Iteration 10\n",
      "Logistic Regression F-1 Score: 0.400000 Recall: 0.333333 AUC: 0.633772\n",
      "Iteration 11\n",
      "Logistic Regression F-1 Score: 0.480000 Recall: 0.428571 AUC: 0.681391\n",
      "Iteration 12\n",
      "Logistic Regression F-1 Score: 0.666667 Recall: 0.571429 AUC: 0.772556\n",
      "Iteration 13\n",
      "Logistic Regression F-1 Score: 0.727273 Recall: 0.571429 AUC: 0.785714\n",
      "Iteration 14\n",
      "Logistic Regression F-1 Score: 0.454545 Recall: 0.357143 AUC: 0.658835\n",
      "Iteration 15\n",
      "Logistic Regression F-1 Score: 0.461538 Recall: 0.428571 AUC: 0.674812\n",
      "Logistic Regression outer test: recall mean: 0.534574 (std: 0.150910)\n",
      "Time:  6107.056003065009\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "caffeine.on(display=True)\n",
    "log4_outer_results = list()\n",
    "log4_best_recall=float('-inf')\n",
    "log4_best_parameters={}\n",
    "iter_num=1\n",
    "for train_ix, test_ix in cv_outer.split(X_train_fs, y_train):\n",
    "    print('Iteration',iter_num)\n",
    "    iter_num +=1\n",
    "    # Split data for inner \n",
    "    X_traini, X_testi = X_train_fs.iloc[train_ix, :], X_train_fs.iloc[test_ix, :]\n",
    "    y_traini, y_testi = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "    #Scaling numeric features\n",
    "    scalor.fit(X_traini[continuous])\n",
    "    X_traini.update(pd.DataFrame(scalor.transform(X_traini[continuous]),index=X_traini[continuous].index,columns=continuous))\n",
    "    X_testi.update(pd.DataFrame(scalor.transform(X_testi[continuous]),index=X_testi[continuous].index,columns=continuous))\n",
    "    #Logistic Regression\n",
    "    result=log_exhaustive.fit(X_traini, y_traini)\n",
    "    best_model = result.best_estimator_\n",
    "    y_pred = best_model.predict(X_testi)\n",
    "    f1score=f1_score(y_testi,y_pred)\n",
    "    recall=recall_score(y_testi,y_pred)\n",
    "    auc=roc_auc_score(y_testi,y_pred)\n",
    "    if recall >= log4_best_recall:\n",
    "        log4_best_recall=recall\n",
    "        log4_best_parameters=result.best_params_\n",
    "    log4_outer_results.append([recall,f1score,auc])\n",
    "    print(\"Logistic Regression F-1 Score: %f Recall: %f AUC: %f\" %(f1score,recall,auc))\n",
    "log4_mean=np.mean(log3_outer_results)\n",
    "print('Logistic Regression outer test: recall mean: %f (std: %f)' % (log4_mean, np.std(log4_outer_results)))\n",
    "caffeine.off()\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log__C': 100,\n",
       " 'log__fit_intercept': True,\n",
       " 'log__max_iter': 10000,\n",
       " 'log__penalty': 'l1',\n",
       " 'log__solver': 'liblinear'}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log4_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
